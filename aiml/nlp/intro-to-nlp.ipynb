{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "- [A Gentle Introduction to NLP](https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863)\n",
    "- [NLP Getting Started Kaggle Exercise](https://www.kaggle.com/philculliton/nlp-getting-started-tutorial)\n",
    "\n",
    "## What is NLP\n",
    "NLP is a branch of artificial intelligence that deals with analyzing, understanding and generating the languages that humans use naturally in order to interface with computers in both written and spoken contexts using natural human languages instead of computer languages.\n",
    "\n",
    "## Applications\n",
    "- Chat Bots\n",
    "- Language Translation\n",
    "- Summarization\n",
    "- Sentiment Analysis\n",
    "- Transcription\n",
    "... and many more\n",
    "\n",
    "## Challenges in NLP\n",
    "- Languages change every day; New Words, Acronyms, New Rules, ...\n",
    "- Dialects\n",
    "- Domain specific & Contextual\n",
    "- Interleving of other language words in a language\n",
    "... and many more\n",
    "\n",
    "## Pre-processing for NLP\n",
    "1. Data Cleansing - remove special characters, symbols, punctuation, html tags<> etc from the raw data which contains no information for the model to learn, these are simply noise in our data.\n",
    "2. Case Conversion - convert to lower case for all characters to bring in uniformity\n",
    "3. Tokensiation - process of breaking up text document into individual words called tokens.\n",
    "4. Stop Words Removal - common words that do not contribute much of the information in a text document. Words like ‘the’, ‘is’, ‘a’ have less value and add noise to the text data.\n",
    "5. Stemming - process of reducing a word to its stem/root word. It reduces inflection in words (e.g. ‘help’, ’helping’, ’helped’, ’helpful’) to their root form (e.g. ‘help’).\n",
    "6. Lemmatization - same thing as stemming, converting a word to its root form but with one difference i.e., the root word in this case belongs to a valid word in the language. For example the word caring would map to ‘care’ and not ‘car’ as the in case of stemming.\n",
    "\n",
    "## N-Grams\n",
    "The combination of multiple words used together, Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "N-grams can be used when we want to preserve sequence information in the document, like what word is likely to follow the given one. Unigrams don’t contain any sequence information because each word is taken individually.\n",
    "\n",
    "## Text Data Vectorization\n",
    "The process of converting text into numbers is called text data vectorization. Now after text preprocessing, we need to numerically represent text data i.e., encoding the data in numbers which can be further used by algorithms.\n",
    "\n",
    "### Bag of Words\n",
    "It is one of the simplest text vectorization techniques. The intuition behind BOW is that two sentences are said to be similar if they contain similar set of words. BOW constructs a dictionary of d unique words in corpus(collection of all the tokens in data). \n",
    "\n",
    "BOW uses Tokenized Words\n",
    "\n",
    "### TF-IDF\n",
    "Stands for Term Frequency(TF)-Inverse Document Frequency. \n",
    "Term Frequency defines the probability of finding a word in the document. \n",
    "- Term Frequency(wi, dj) = Number of times wi occurs in dj/Total number of words in dj\n",
    "    - wi is Word i\n",
    "    - dj is Document j\n",
    "\n",
    "Inverse Document Frequency defines how unique is the word in the total corpus.\n",
    "- IDF(wi, Dc) = log(N/ni)\n",
    "    - wi is Word i\n",
    "    - Dc is all documents in the corpus\n",
    "    - N is total number of documents\n",
    "    - ni is document which contains word i\n",
    "    - If wi is more frequent in the corpus then IDF value decreases.\n",
    "    - If wi is not frequent which means ni decreases and hence IDF value increases.\n",
    "\n",
    "TF-IDF is the multiplication of TF and IDF values. It gives more weightage to words which occurs more in the document and less in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of tweets about disasters\n",
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6136</th>\n",
       "      <td>8754</td>\n",
       "      <td>siren</td>\n",
       "      <td>New York</td>\n",
       "      <td>WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2078</td>\n",
       "      <td>casualty</td>\n",
       "      <td>Trinity, Bailiwick of Jersey</td>\n",
       "      <td>@ScriptetteSar @katiecool447 btw the 30th is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>9989</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>but i love kaylen ??</td>\n",
       "      <td>I hope this tsunami clears b4 i have to walk o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>1214</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>The ?? below ???</td>\n",
       "      <td>@BubblyCuteOne ?????????? ok ok okayyyyyy Ima ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>244</td>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>nyc</td>\n",
       "      <td>The shooting or the airplane accident  https:/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>8346</td>\n",
       "      <td>ruin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I ruin everything ????</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>9495</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>peshawar pakistan</td>\n",
       "      <td>@OfficialMqm you are terrorist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>4669</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>10855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>9134</td>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>#GRupdates Pic of 16yr old PKK suicide bomber ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword                      location  \\\n",
       "6136   8754                siren                      New York   \n",
       "1442   2078             casualty  Trinity, Bailiwick of Jersey   \n",
       "6964   9989              tsunami          but i love kaylen ??   \n",
       "836    1214             blizzard              The ?? below ???   \n",
       "169     244  airplane%20accident                           nyc   \n",
       "5840   8346                 ruin                           NaN   \n",
       "6630   9495            terrorist            peshawar pakistan    \n",
       "3248   4669             engulfed                       Bahrain   \n",
       "7600  10855                  NaN                           NaN   \n",
       "6391   9134       suicide%20bomb                       Nigeria   \n",
       "\n",
       "                                                   text  target  \n",
       "6136  WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...       0  \n",
       "1442  @ScriptetteSar @katiecool447 btw the 30th is a...       1  \n",
       "6964  I hope this tsunami clears b4 i have to walk o...       1  \n",
       "836   @BubblyCuteOne ?????????? ok ok okayyyyyy Ima ...       1  \n",
       "169   The shooting or the airplane accident  https:/...       1  \n",
       "5840                             I ruin everything ????       0  \n",
       "6630                     @OfficialMqm you are terrorist       0  \n",
       "3248  He came to a land which was engulfed in tribal...       1  \n",
       "7600  Evacuation order lifted for town of Roosevelt:...       1  \n",
       "6391  #GRupdates Pic of 16yr old PKK suicide bomber ...       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@JustineJayyy OHGOD XD I didn't mean it so =P But you have that fire truck in the back of you to make up for it so you good xD\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a disaster\n",
    "import random\n",
    "train_df[train_df[\"target\"] == 0][\"text\"].values[random.randint(0, len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wreckage 'Conclusively Confirmed' as From MH370: Malaysia PM: Investigators and the families of those who were... http://t.co/MSsq0sVnBM\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Disaster\n",
    "train_df[train_df[\"target\"] == 1][\"text\"].values[random.randint(0, len(train_df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Find the issue in the above code*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory behind the model we'll build in this notebook is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n",
    "\n",
    "We'll use scikit-learn's CountVectorizer to count the words in each tweet and turn them into data our machine learning model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Our Deeds are the Reason of this #earthquake M...\n",
       "1               Forest fire near La Ronge Sask. Canada\n",
       "2    All residents asked to 'shelter in place' are ...\n",
       "3    13,000 people receive #wildfires evacuation or...\n",
       "4    Just got sent this photo from Ruby #Alaska as ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 54)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_train_vectors.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 54)\n",
      "[[1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
    "print(example_train_vectors[3].todense().shape)\n",
    "print(example_train_vectors[3].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tells us that:\n",
    "\n",
    "There are 54 unique words (or \"tokens\") in the first five tweets.\n",
    "The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\n",
    "Now let's create vectors for all of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 21637)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x21637 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_vectors[0].todense()\n",
    "x[x != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "train_vectors_tfidf = tfidf_vectorizer.fit_transform(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 21637)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors_tfidf[0].todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x21637 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors_tfidf[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.20827049, 0.36169348, 0.1893734 , 0.4187282 , 0.29044218,\n",
       "         0.4187282 , 0.25921234, 0.11990035, 0.2530913 , 0.32654636,\n",
       "         0.10169057, 0.18001477, 0.24477443]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_vectors_tfidf[0].todense()\n",
    "x[x != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words contained in each tweet are a good indicator of whether they're about a real disaster or not. \n",
    "# The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n",
    "\n",
    "# Assuming Linear Connection, let's build a linear model\n",
    "clf = linear_model.RidgeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6025641 , 0.50168919, 0.56940063, 0.50781969, 0.67275495])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62962963, 0.55507372, 0.64457332, 0.59444444, 0.72337043])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_tfidf = model_selection.cross_val_score(clf, train_vectors_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/drs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/drs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/drs/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/drs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'careless'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"careless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(tweet):\n",
    "    \n",
    "    # Cleansing\n",
    "    tweet = re.sub(re.compile('<.*?>'), '', tweet)  # remove html tags\n",
    "    tweet = re.sub('[^A-Za-z0-9]+',' ', tweet)\n",
    "\n",
    "    # lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # tokenization\n",
    "    tokens = word_tokenize(tweet)\n",
    "\n",
    "    # stop words removal\n",
    "    tweet = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    tweet = [lemmatizer.lemmatize(word) for word in tweet]\n",
    "\n",
    "    # join words back\n",
    "    tweet = ' '.join(tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"target\"] == 1][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deed reason earthquake may allah forgive u'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessing(train_df[train_df[\"target\"] == 1][\"text\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text'] = train_df['text'].apply(lambda tweet: data_preprocessing(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>5476</td>\n",
       "      <td>flames</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'll cry until my pity party's in flames ????</td>\n",
       "      <td>0</td>\n",
       "      <td>cry pity party flame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>5196</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'Among other main factors behind pedestrian fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>among main factor behind pedestrian fatality p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>5674</td>\n",
       "      <td>floods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Have you ever remembered an old song something...</td>\n",
       "      <td>0</td>\n",
       "      <td>ever remembered old song something heared year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>1774</td>\n",
       "      <td>buildings%20on%20fire</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Video:  Fire burns two apartment buildings and...</td>\n",
       "      <td>1</td>\n",
       "      <td>video fire burn two apartment building blow ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>5850</td>\n",
       "      <td>hailstorm</td>\n",
       "      <td>Washington State</td>\n",
       "      <td>We The Free Hailstorm Maxi http://t.co/ERWs6IELdG</td>\n",
       "      <td>1</td>\n",
       "      <td>free hailstorm maxi http co erws6ieldg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                keyword          location  \\\n",
       "3848  5476                 flames               NaN   \n",
       "3647  5196             fatalities               NaN   \n",
       "3995  5674                 floods               NaN   \n",
       "1233  1774  buildings%20on%20fire     New Hampshire   \n",
       "4117  5850              hailstorm  Washington State   \n",
       "\n",
       "                                                   text  target  \\\n",
       "3848      I'll cry until my pity party's in flames ????       0   \n",
       "3647  'Among other main factors behind pedestrian fa...       1   \n",
       "3995  Have you ever remembered an old song something...       0   \n",
       "1233  Video:  Fire burns two apartment buildings and...       1   \n",
       "4117  We The Free Hailstorm Maxi http://t.co/ERWs6IELdG       1   \n",
       "\n",
       "                                         processed_text  \n",
       "3848                               cry pity party flame  \n",
       "3647  among main factor behind pedestrian fatality p...  \n",
       "3995  ever remembered old song something heared year...  \n",
       "1233  video fire burn two apartment building blow ca...  \n",
       "4117             free hailstorm maxi http co erws6ieldg  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_tfidf_processed = tfidf_vectorizer.fit_transform(train_df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20153)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors_tfidf_processed[0].todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58666667, 0.52413793, 0.59369818, 0.5418251 , 0.72143975])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, train_vectors_tfidf_processed, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5891182 , 0.60367893, 0.64479081, 0.61056401, 0.75154321])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf_nb, train_vectors_tfidf_processed, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nb.fit(train_vectors_tfidf_processed, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9018783659529752\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_nb.predict(train_vectors_tfidf_processed)\n",
    "print('Train Accuracy:', accuracy_score(train_df[\"target\"], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58553792, 0.54888508, 0.62795941])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr = LogisticRegression()\n",
    "scores = model_selection.cross_val_score(clf_lr, train_vectors_tfidf_processed, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ec691e10227250387cc3792844e063eb337938bf091c7ae5f8c4a26b5c484a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aiml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
